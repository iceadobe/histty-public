[{"categories":null,"content":"Notes ","date":"2025-11-26","objectID":"/logs/2025/11/2025-11-26-daily-notes/:1:0","tags":["logs/2025-11"],"title":"Daily Logs - November 26, 2025","uri":"/logs/2025/11/2025-11-26-daily-notes/"},{"categories":null,"content":"Godot Signals Signals are Godot’s implementation of the observer pattern Use signal signal_name(param1, param2) to declare Emit with emit_signal(\"signal_name\", arg1, arg2) Connect with object.connect(\"signal_name\", self, \"_on_signal_received\") Super useful for decoupling game systems (e.g., player health changes triggering UI updates) ","date":"2025-11-26","objectID":"/logs/2025/11/2025-11-26-daily-notes/:1:1","tags":["logs/2025-11"],"title":"Daily Logs - November 26, 2025","uri":"/logs/2025/11/2025-11-26-daily-notes/"},{"categories":null,"content":"AI Pathfinding Started implementing AI for enemy AI Key insight: the heuristic function matters a lot for performance Using Manhattan distance for grid-based movement (no diagonals) Need to cache paths and only recalculate when player moves significantly Found a great visualization tool: pathfinding.js ","date":"2025-11-26","objectID":"/logs/2025/11/2025-11-26-daily-notes/:1:2","tags":["logs/2025-11"],"title":"Daily Logs - November 26, 2025","uri":"/logs/2025/11/2025-11-26-daily-notes/"},{"categories":null,"content":"Go Interfaces Interfaces in Go are implicit - no need to declare implementation Any type that implements all methods automatically satisfies the interface Keep interfaces small (often just 1-2 methods) “Accept interfaces, return structs” is a common pattern The empty interface interface{} accepts any type (like any in newer Go versions) ","date":"2025-11-26","objectID":"/logs/2025/11/2025-11-26-daily-notes/:1:3","tags":["logs/2025-11"],"title":"Daily Logs - November 26, 2025","uri":"/logs/2025/11/2025-11-26-daily-notes/"},{"categories":null,"content":"Random Discovered itch.io’s devlog feature - might cross-post there Need to set up a better note-taking workflow (currently using Obsidian + manual conversion) Read an interesting article about roguelike design patterns ","date":"2025-11-26","objectID":"/logs/2025/11/2025-11-26-daily-notes/:1:4","tags":["logs/2025-11"],"title":"Daily Logs - November 26, 2025","uri":"/logs/2025/11/2025-11-26-daily-notes/"},{"categories":null,"content":"Tomorrow Implement basic AI pathfinding for enemies Refactor player movement to use signals Write up a proper blog post about system design ","date":"2025-11-26","objectID":"/logs/2025/11/2025-11-26-daily-notes/:1:5","tags":["logs/2025-11"],"title":"Daily Logs - November 26, 2025","uri":"/logs/2025/11/2025-11-26-daily-notes/"},{"categories":null,"content":"Deploying Large Language Models (LLMs) effectively requires a robust architecture that can handle high concurrency, manage GPU resources efficiently, and scale dynamically. In this post, I’ll walk through a production-ready setup for hosting open-source models (like Llama 3 or Mistral) on AWS using Ray Serve for orchestration and FastAPI as the interface. ","date":"2025-11-22","objectID":"/blog/deploying-llms-aws-ray-serve/:0:0","tags":["devops"],"title":"Scalable LLM Deployment on AWS with Ray Serve and FastAPI","uri":"/blog/deploying-llms-aws-ray-serve/"},{"categories":null,"content":"The Architecture The stack consists of: Infrastructure: AWS EC2 instances (g5.xlarge or similar GPU-optimized instances) Orchestration: Ray Cluster (Head node + Worker nodes) Serving: Ray Serve wrapping a FastAPI application Model Engine: vLLM for high-throughput inference ","date":"2025-11-22","objectID":"/blog/deploying-llms-aws-ray-serve/:1:0","tags":["devops"],"title":"Scalable LLM Deployment on AWS with Ray Serve and FastAPI","uri":"/blog/deploying-llms-aws-ray-serve/"},{"categories":null,"content":"Why Ray Serve? Ray Serve excels at “model composition” and scaling. Unlike simple Docker containers, Ray allows us to: Scale independently: Scale the model replicas separately from the API handling logic. Batching: Native support for dynamic request batching to maximize GPU utilization. Pipeline composition: Easily chain multiple models or pre/post-processing steps. ","date":"2025-11-22","objectID":"/blog/deploying-llms-aws-ray-serve/:1:1","tags":["devops"],"title":"Scalable LLM Deployment on AWS with Ray Serve and FastAPI","uri":"/blog/deploying-llms-aws-ray-serve/"},{"categories":null,"content":"Configuration Here is a simplified serve_config.yaml to get started. This configuration defines a deployment that autoscales based on request load. proxy_location: EveryNode http_options: host: 0.0.0.0 port: 8000 applications: - name: llm_app route_prefix: / import_path: app:deployment runtime_env: pip: - vllm - fastapi deployments: - name: VLLMDeployment autoscaling_config: min_replicas: 1 max_replicas: 4 target_num_ongoing_requests_per_replica: 10 ray_actor_options: num_gpus: 1\r","date":"2025-11-22","objectID":"/blog/deploying-llms-aws-ray-serve/:2:0","tags":["devops"],"title":"Scalable LLM Deployment on AWS with Ray Serve and FastAPI","uri":"/blog/deploying-llms-aws-ray-serve/"},{"categories":null,"content":"The FastAPI Wrapper We wrap the vLLM engine in a FastAPI app to expose standard REST endpoints. This allows easy integration with existing frontend applications or services. from fastapi import FastAPI from ray import serve from vllm import AsyncLLMEngine, SamplingParams app = FastAPI() @serve.deployment(num_gpus=1) @serve.ingress(app) class VLLMDeployment: def __init__(self): # Initialize vLLM engine self.engine = AsyncLLMEngine.from_engine_args(...) @app.post(\"/generate\") async def generate(self, prompt: str): sampling_params = SamplingParams(temperature=0.7, max_tokens=100) results = await self.engine.generate(prompt, sampling_params, ...) return {\"text\": results[0].outputs[0].text} deployment = VLLMDeployment.bind()\r","date":"2025-11-22","objectID":"/blog/deploying-llms-aws-ray-serve/:3:0","tags":["devops"],"title":"Scalable LLM Deployment on AWS with Ray Serve and FastAPI","uri":"/blog/deploying-llms-aws-ray-serve/"},{"categories":null,"content":"Deployment on AWS Cluster Setup: Use the Ray Cluster Launcher to provision EC2 instances. Define your cluster configuration in a cluster.yaml file, specifying the instance types (e.g., g5.xlarge for workers). Deploy: Run ray up cluster.yaml to start the cluster. Serve: Submit your serve application using serve run serve_config.yaml. ","date":"2025-11-22","objectID":"/blog/deploying-llms-aws-ray-serve/:4:0","tags":["devops"],"title":"Scalable LLM Deployment on AWS with Ray Serve and FastAPI","uri":"/blog/deploying-llms-aws-ray-serve/"},{"categories":null,"content":"Monitoring and Optimization Ray provides a built-in dashboard to monitor actor status, GPU usage, and request latency. For production, integrate this with Prometheus and Grafana to track: Queue Latency: Time requests spend waiting for a replica. GPU Utilization: Ensure you aren’t under-provisioning expensive hardware. Token Throughput: Measure tokens/second to benchmark performance. By leveraging Ray Serve with AWS, we create a flexible, scalable inference platform that avoids the vendor lock-in of managed services while providing full control over the serving infrastructure. ","date":"2025-11-22","objectID":"/blog/deploying-llms-aws-ray-serve/:5:0","tags":["devops"],"title":"Scalable LLM Deployment on AWS with Ray Serve and FastAPI","uri":"/blog/deploying-llms-aws-ray-serve/"}]