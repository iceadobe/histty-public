<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Scalable LLM Deployment on AWS with Ray Serve and FastAPI - Himalaya's Dev Journal</title><meta name=Description content="Personal journal, game dev blog, and project portfolio"><meta property="og:url" content="https://histty.in/blog/deploying-llms-aws-ray-serve/"><meta property="og:site_name" content="Himalaya's Dev Journal"><meta property="og:title" content="Scalable LLM Deployment on AWS with Ray Serve and FastAPI"><meta property="og:description" content="A guide to deploying open-source LLMs using Ray Serve for orchestration and FastAPI for the serving layer on AWS infrastructure."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-11-22T00:00:00+00:00"><meta property="article:modified_time" content="2026-01-07T15:49:03+05:30"><meta property="article:tag" content="Devops"><meta property="og:image" content="https://histty.in/logo.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://histty.in/logo.png"><meta name=twitter:title content="Scalable LLM Deployment on AWS with Ray Serve and FastAPI"><meta name=twitter:description content="A guide to deploying open-source LLMs using Ray Serve for orchestration and FastAPI for the serving layer on AWS infrastructure."><meta name=application-name content="Himalaya's Dev Journal"><meta name=apple-mobile-web-app-title content="Himalaya's Dev Journal"><meta name=referrer content="no-referrer"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://histty.in/blog/deploying-llms-aws-ray-serve/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=/lib/fontawesome-free/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/fontawesome-free/css/all.min.css></noscript><link rel=preload href=/lib/animate/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><meta name=google-site-verification content="Ely7KvxrOTEG3hm92x9JB0nnHO4jZ2GDjiIEOoAub04"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Scalable LLM Deployment on AWS with Ray Serve and FastAPI","inLanguage":"en-us","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/histty.in\/blog\/deploying-llms-aws-ray-serve\/"},"image":["https:\/\/histty.in\/images\/histty-logo.png"],"genre":"blog","keywords":"devops","wordcount":442,"url":"https:\/\/histty.in\/blog\/deploying-llms-aws-ray-serve\/","datePublished":"2025-11-22T00:00:00+00:00","dateModified":"2026-01-07T15:49:03+05:30","publisher":{"@type":"Organization","name":" Himalaya"},"author":{"@type":"Person","name":" Himalaya"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script>const query=window.matchMedia("(prefers-color-scheme: dark)");function applyTheme(){let e=window.localStorage?.getItem("theme")||"dark",t=e==="dark"||e==="auto"&&query.matches;document.body.setAttribute("theme",t?"dark":"light"),document.body.setAttribute("cfg-theme",e)}applyTheme(),query.addEventListener("change",applyTheme)</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="Himalaya's Dev Journal"><img src=/images/title.png class=logo alt="Himalaya's Dev Journal">Dev Journal</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/>Home </a><a class=menu-item href=/projects/>Projects </a><a class=menu-item href=/blog/>Blog </a><a class=menu-item href=/logs/>Logs </a><a class=menu-item href=/now/>Now </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i>
</span></span><a href=javascript:void(0); class="menu-item theme-switch" title><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Himalaya's Dev Journal"><img src=/images/title.png class=logo alt="Himalaya's Dev Journal">Dev Journal</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile></a></div><a class=menu-item href=/ title>Home</a><a class=menu-item href=/projects/ title>Projects</a><a class=menu-item href=/blog/ title>Blog</a><a class=menu-item href=/logs/ title>Logs</a><a class=menu-item href=/now/ title>Now</a><a href=javascript:void(0); class="menu-item theme-switch" title>
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><article class="page single"><h1 class=single-title itemprop="name headline">Scalable LLM Deployment on AWS with Ray Serve and FastAPI</h1><div class=post-meta><div class=post-meta-line><span class=post-author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i> Himalaya</span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2025-11-22>November 22, 2025</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;</div><div class="post-meta-line post-meta-taxonomy"><span class=post-tags-group><i class="fas fa-tags fa-fw" aria-hidden=true></i><a href=/tags/devops/ class="meta-badge meta-tag">Devops</a></span><span class=post-domains-group>
<i class="fas fa-folder fa-fw" aria-hidden=true></i><a href=/domains/engineering/ class="meta-badge meta-domain">Engineering</a><a href=/domains/ai/ml/ class="meta-badge meta-domain">AI/ML</a></span><span class=post-tech-group>
<i class="fas fa-code fa-fw" aria-hidden=true></i><a href=/tech/aws/ class="meta-badge meta-tech">aws</a><a href=/tech/ray-serve/ class="meta-badge meta-tech">ray-serve</a><a href=/tech/fastapi/ class="meta-badge meta-tech">fastapi</a><a href=/tech/python/ class="meta-badge meta-tech">python</a></span></div></div><div class=content id=content><p>Deploying Large Language Models (LLMs) effectively requires a robust architecture that can handle high concurrency, manage GPU resources efficiently, and scale dynamically. In this post, I&rsquo;ll walk through a production-ready setup for hosting open-source models (like Llama 3 or Mistral) on AWS using <strong>Ray Serve</strong> for orchestration and <strong>FastAPI</strong> as the interface.</p><h2 id=the-architecture>The Architecture</h2><p>The stack consists of:</p><ul><li><strong>Infrastructure</strong>: AWS EC2 instances (g5.xlarge or similar GPU-optimized instances)</li><li><strong>Orchestration</strong>: Ray Cluster (Head node + Worker nodes)</li><li><strong>Serving</strong>: Ray Serve wrapping a FastAPI application</li><li><strong>Model Engine</strong>: vLLM for high-throughput inference</li></ul><h3 id=why-ray-serve>Why Ray Serve?</h3><p>Ray Serve excels at &ldquo;model composition&rdquo; and scaling. Unlike simple Docker containers, Ray allows us to:</p><ol><li><strong>Scale independently</strong>: Scale the model replicas separately from the API handling logic.</li><li><strong>Batching</strong>: Native support for dynamic request batching to maximize GPU utilization.</li><li><strong>Pipeline composition</strong>: Easily chain multiple models or pre/post-processing steps.</li></ol><h2 id=configuration>Configuration</h2><p>Here is a simplified <code>serve_config.yaml</code> to get started. This configuration defines a deployment that autoscales based on request load.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>proxy_location</span><span class=p>:</span><span class=w> </span><span class=l>EveryNode</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>http_options</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>host</span><span class=p>:</span><span class=w> </span><span class=m>0.0.0.0</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>8000</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>applications</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>llm_app</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>route_prefix</span><span class=p>:</span><span class=w> </span><span class=l>/</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>import_path</span><span class=p>:</span><span class=w> </span><span class=l>app:deployment</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>runtime_env</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>pip</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=l>vllm</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=l>fastapi</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>deployments</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>VLLMDeployment</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>autoscaling_config</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>min_replicas</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>max_replicas</span><span class=p>:</span><span class=w> </span><span class=m>4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>target_num_ongoing_requests_per_replica</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>ray_actor_options</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>num_gpus</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><h2 id=the-fastapi-wrapper>The FastAPI Wrapper</h2><p>We wrap the vLLM engine in a FastAPI app to expose standard REST endpoints. This allows easy integration with existing frontend applications or services.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>fastapi</span> <span class=kn>import</span> <span class=n>FastAPI</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>ray</span> <span class=kn>import</span> <span class=n>serve</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>vllm</span> <span class=kn>import</span> <span class=n>AsyncLLMEngine</span><span class=p>,</span> <span class=n>SamplingParams</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>app</span> <span class=o>=</span> <span class=n>FastAPI</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@serve.deployment</span><span class=p>(</span><span class=n>num_gpus</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nd>@serve.ingress</span><span class=p>(</span><span class=n>app</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>VLLMDeployment</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Initialize vLLM engine</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>engine</span> <span class=o>=</span> <span class=n>AsyncLLMEngine</span><span class=o>.</span><span class=n>from_engine_args</span><span class=p>(</span><span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@app.post</span><span class=p>(</span><span class=s2>&#34;/generate&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>async</span> <span class=k>def</span> <span class=nf>generate</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>prompt</span><span class=p>:</span> <span class=nb>str</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>sampling_params</span> <span class=o>=</span> <span class=n>SamplingParams</span><span class=p>(</span><span class=n>temperature</span><span class=o>=</span><span class=mf>0.7</span><span class=p>,</span> <span class=n>max_tokens</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>results</span> <span class=o>=</span> <span class=k>await</span> <span class=bp>self</span><span class=o>.</span><span class=n>engine</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=n>prompt</span><span class=p>,</span> <span class=n>sampling_params</span><span class=p>,</span> <span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span><span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=n>results</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>outputs</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>text</span><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>deployment</span> <span class=o>=</span> <span class=n>VLLMDeployment</span><span class=o>.</span><span class=n>bind</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=deployment-on-aws>Deployment on AWS</h2><ol><li><strong>Cluster Setup</strong>: Use the Ray Cluster Launcher to provision EC2 instances. Define your cluster configuration in a <code>cluster.yaml</code> file, specifying the instance types (e.g., <code>g5.xlarge</code> for workers).</li><li><strong>Deploy</strong>: Run <code>ray up cluster.yaml</code> to start the cluster.</li><li><strong>Serve</strong>: Submit your serve application using <code>serve run serve_config.yaml</code>.</li></ol><h2 id=monitoring-and-optimization>Monitoring and Optimization</h2><p>Ray provides a built-in dashboard to monitor actor status, GPU usage, and request latency. For production, integrate this with <strong>Prometheus</strong> and <strong>Grafana</strong> to track:</p><ul><li><strong>Queue Latency</strong>: Time requests spend waiting for a replica.</li><li><strong>GPU Utilization</strong>: Ensure you aren&rsquo;t under-provisioning expensive hardware.</li><li><strong>Token Throughput</strong>: Measure tokens/second to benchmark performance.</li></ul><p>By leveraging Ray Serve with AWS, we create a flexible, scalable inference platform that avoids the vendor lock-in of managed services while providing full control over the serving infrastructure.</p></div><div class=post-footer><div class=post-nav></div></div></article><div id=comments class=post-comments><div id=giscus_container style="max-width:760px;margin:0 auto"><script id=giscus_script src=https://giscus.app/client.js data-repo=iceadobe/histty-public data-repo-id=R_kgDONrz3lg data-category=Comments data-category-id=DIC_kwDONrz3ls4C0uEQ data-mapping=url data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=https://histty.in/css/giscus-light.min.0b0c7a0b1168ab34e1979b3606217f42fd9836543ab7b0c8e3c74cc34f9b70dc.css data-lang=en data-loading=lazy crossorigin=anonymous async></script></div><script>(function(){function t(){var e=document.documentElement,t=document.body;return e&&(e.getAttribute("theme")||e.getAttribute("data-theme")||e.getAttribute("cfg-theme"))||t&&(t.getAttribute("theme")||t.getAttribute("data-theme")||t.getAttribute("cfg-theme"))||(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")}var e,n="https://histty.in/css/giscus-light.min.0b0c7a0b1168ab34e1979b3606217f42fd9836543ab7b0c8e3c74cc34f9b70dc.css",s="https://histty.in/css/giscus-dark.min.5fc6676adb86885ff87049c03f1238eb987abdbab33ee305c57b40bd810ef61e.css";function o(e){return e==="dark"?s:n}e=new MutationObserver(function(e){var n,s,i=e.some(function(e){return e.type==="attributes"&&(e.attributeName==="theme"||e.attributeName==="data-theme"||e.attributeName==="cfg-theme")});if(!i)return;s=o(t()),n=document.querySelector("link#giscus-theme"),n&&(n.href=s)}),document.documentElement&&e.observe(document.documentElement,{attributes:!0}),document.body&&e.observe(document.body,{attributes:!0})})()</script></div></div></main><footer class=footer><div class=footer-container><div class=footer-line>All content Â© 2025 Himalaya Ghimire. All rights reserved.</div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i></a></div><div id=fixed-buttons-hidden><a href=# id=view-comments class=fixed-button title><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><script src=/lib/autocomplete/autocomplete.min.js></script><script src=/lib/lunr/lunr.min.js></script><script src=/lib/lazysizes/lazysizes.min.js></script><script src=/lib/clipboard/clipboard.min.js></script><script src=/lib/sharer/sharer.min.js></script><script>window.config={search:{highlightTag:"em",lunrIndexURL:"/index.json",maxResultLength:10,noResultsFound:"",snippetLength:30,type:"lunr"}}</script><script src=/js/theme.min.js></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-HMV10PTQ4L")}</script><script src="https://www.googletagmanager.com/gtag/js?id=G-HMV10PTQ4L" async></script></body></html>