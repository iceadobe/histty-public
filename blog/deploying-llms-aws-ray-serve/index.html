<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Scalable LLM Deployment on AWS with Ray Serve and FastAPI - Himalaya's Dev Journal</title><meta name=Description content="Personal journal, game dev blog, and project portfolio"><meta property="og:url" content="https://histty.in/blog/deploying-llms-aws-ray-serve/"><meta property="og:site_name" content="Himalaya's Dev Journal"><meta property="og:title" content="Scalable LLM Deployment on AWS with Ray Serve and FastAPI"><meta property="og:description" content="A guide to deploying open-source LLMs using Ray Serve for orchestration and FastAPI for the serving layer on AWS infrastructure."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-11-22T00:00:00+00:00"><meta property="article:modified_time" content="2025-11-23T13:44:13+05:30"><meta property="article:tag" content="Devops"><meta property="og:image" content="https://histty.in/logo.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://histty.in/logo.png"><meta name=twitter:title content="Scalable LLM Deployment on AWS with Ray Serve and FastAPI"><meta name=twitter:description content="A guide to deploying open-source LLMs using Ray Serve for orchestration and FastAPI for the serving layer on AWS infrastructure."><meta name=application-name content="Himalaya's Dev Journal"><meta name=apple-mobile-web-app-title content="Himalaya's Dev Journal"><meta name=referrer content="no-referrer"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://histty.in/blog/deploying-llms-aws-ray-serve/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><meta name=google-site-verification content="Ely7KvxrOTEG3hm92x9JB0nnHO4jZ2GDjiIEOoAub04"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Scalable LLM Deployment on AWS with Ray Serve and FastAPI","inLanguage":"en-us","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/histty.in\/blog\/deploying-llms-aws-ray-serve\/"},"image":["https:\/\/histty.in\/images\/histty-logo.png"],"genre":"blog","keywords":"devops","wordcount":400,"url":"https:\/\/histty.in\/blog\/deploying-llms-aws-ray-serve\/","datePublished":"2025-11-22T00:00:00+00:00","dateModified":"2025-11-23T13:44:13+05:30","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"xxxx"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script>const query=window.matchMedia("(prefers-color-scheme: dark)");function applyTheme(){let e=window.localStorage?.getItem("theme")||"dark",t=e==="dark"||e==="auto"&&query.matches;document.body.setAttribute("theme",t?"dark":"light"),document.body.setAttribute("cfg-theme",e)}applyTheme(),query.addEventListener("change",applyTheme)</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="Himalaya's Dev Journal"><img class="lazyload logo" src=/svg/loading.min.svg data-src=/images/histty-logo.png data-srcset="/images/histty-logo.png, /images/histty-logo.png 1.5x, /images/histty-logo.png 2x" data-sizes=auto alt=/images/histty-logo.png title=/images/histty-logo.png>histty.in</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/>Home </a><a class=menu-item href=/projects/>Projects </a><a class=menu-item href=/blog/>Blog </a><a class=menu-item href=/logs/>Logs </a><a class=menu-item href=/discover/>Discover </a><a class=menu-item href=/about/>About </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i>
</span></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Himalaya's Dev Journal"><img class="lazyload logo" src=/svg/loading.min.svg data-src=/images/histty-logo.png data-srcset="/images/histty-logo.png, /images/histty-logo.png 1.5x, /images/histty-logo.png 2x" data-sizes=auto alt=/images/histty-logo.png title=/images/histty-logo.png>histty.in</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/ title>Home</a><a class=menu-item href=/projects/ title>Projects</a><a class=menu-item href=/blog/ title>Blog</a><a class=menu-item href=/logs/ title>Logs</a><a class=menu-item href=/discover/ title>Discover</a><a class=menu-item href=/about/ title>About</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Scalable LLM Deployment on AWS with Ray Serve and FastAPI</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>xxxx</a></span>&nbsp;<span class=post-category>included in <a href=/domains/engineering/><i class="far fa-folder fa-fw" aria-hidden=true></i>Engineering</a>&nbsp;<a href=/domains/ai/ml/><i class="far fa-folder fa-fw" aria-hidden=true></i>AI/ML</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2025-11-22>2025-11-22</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;400 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;2 minutes&nbsp;</div><div class=post-meta-line style=margin-top:.5rem><span class=post-tech><i class="fas fa-tools fa-fw" aria-hidden=true></i>&nbsp;Built with:&nbsp;<a href=/tech/aws/>Aws</a>,&nbsp;<a href=/tech/ray-serve/>Ray-Serve</a>,&nbsp;<a href=/tech/fastapi/>Fastapi</a>,&nbsp;<a href=/tech/python/>Python</a></span></div><div class=post-meta-line><span class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/devops/>Devops</a></span></div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#the-architecture>The Architecture</a><ul><li><a href=#why-ray-serve>Why Ray Serve?</a></li></ul></li><li><a href=#configuration>Configuration</a></li><li><a href=#the-fastapi-wrapper>The FastAPI Wrapper</a></li><li><a href=#deployment-on-aws>Deployment on AWS</a></li><li><a href=#monitoring-and-optimization>Monitoring and Optimization</a></li></ul></nav></div></div><div class=content id=content><p>Deploying Large Language Models (LLMs) effectively requires a robust architecture that can handle high concurrency, manage GPU resources efficiently, and scale dynamically. In this post, I&rsquo;ll walk through a production-ready setup for hosting open-source models (like Llama 3 or Mistral) on AWS using <strong>Ray Serve</strong> for orchestration and <strong>FastAPI</strong> as the interface.</p><h2 id=the-architecture>The Architecture</h2><p>The stack consists of:</p><ul><li><strong>Infrastructure</strong>: AWS EC2 instances (g5.xlarge or similar GPU-optimized instances)</li><li><strong>Orchestration</strong>: Ray Cluster (Head node + Worker nodes)</li><li><strong>Serving</strong>: Ray Serve wrapping a FastAPI application</li><li><strong>Model Engine</strong>: vLLM for high-throughput inference</li></ul><h3 id=why-ray-serve>Why Ray Serve?</h3><p>Ray Serve excels at &ldquo;model composition&rdquo; and scaling. Unlike simple Docker containers, Ray allows us to:</p><ol><li><strong>Scale independently</strong>: Scale the model replicas separately from the API handling logic.</li><li><strong>Batching</strong>: Native support for dynamic request batching to maximize GPU utilization.</li><li><strong>Pipeline composition</strong>: Easily chain multiple models or pre/post-processing steps.</li></ol><h2 id=configuration>Configuration</h2><p>Here is a simplified <code>serve_config.yaml</code> to get started. This configuration defines a deployment that autoscales based on request load.</p><div class="code-block code-line-numbers" style="counter-reset:code-block 0"><div class="code-header language-yaml"><span class=code-title><i class="arrow fas fa-angle-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title="Copy to clipboard"><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>proxy_location</span><span class=p>:</span><span class=w> </span><span class=l>EveryNode</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>http_options</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>host</span><span class=p>:</span><span class=w> </span><span class=m>0.0.0.0</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>8000</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>applications</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>llm_app</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>route_prefix</span><span class=p>:</span><span class=w> </span><span class=l>/</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>import_path</span><span class=p>:</span><span class=w> </span><span class=l>app:deployment</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>runtime_env</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>pip</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=l>vllm</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=l>fastapi</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>deployments</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>VLLMDeployment</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>autoscaling_config</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>min_replicas</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>max_replicas</span><span class=p>:</span><span class=w> </span><span class=m>4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>target_num_ongoing_requests_per_replica</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>ray_actor_options</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>num_gpus</span><span class=p>:</span><span class=w> </span><span class=m>1</span></span></span></code></pre></div></div><h2 id=the-fastapi-wrapper>The FastAPI Wrapper</h2><p>We wrap the vLLM engine in a FastAPI app to expose standard REST endpoints. This allows easy integration with existing frontend applications or services.</p><div class="code-block code-line-numbers" style="counter-reset:code-block 0"><div class="code-header language-python"><span class=code-title><i class="arrow fas fa-angle-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title="Copy to clipboard"><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>fastapi</span> <span class=kn>import</span> <span class=n>FastAPI</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>ray</span> <span class=kn>import</span> <span class=n>serve</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>vllm</span> <span class=kn>import</span> <span class=n>AsyncLLMEngine</span><span class=p>,</span> <span class=n>SamplingParams</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>app</span> <span class=o>=</span> <span class=n>FastAPI</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@serve.deployment</span><span class=p>(</span><span class=n>num_gpus</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nd>@serve.ingress</span><span class=p>(</span><span class=n>app</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>VLLMDeployment</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Initialize vLLM engine</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>engine</span> <span class=o>=</span> <span class=n>AsyncLLMEngine</span><span class=o>.</span><span class=n>from_engine_args</span><span class=p>(</span><span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@app.post</span><span class=p>(</span><span class=s2>&#34;/generate&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>async</span> <span class=k>def</span> <span class=nf>generate</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>prompt</span><span class=p>:</span> <span class=nb>str</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>sampling_params</span> <span class=o>=</span> <span class=n>SamplingParams</span><span class=p>(</span><span class=n>temperature</span><span class=o>=</span><span class=mf>0.7</span><span class=p>,</span> <span class=n>max_tokens</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>results</span> <span class=o>=</span> <span class=k>await</span> <span class=bp>self</span><span class=o>.</span><span class=n>engine</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=n>prompt</span><span class=p>,</span> <span class=n>sampling_params</span><span class=p>,</span> <span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span><span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=n>results</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>outputs</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>text</span><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>deployment</span> <span class=o>=</span> <span class=n>VLLMDeployment</span><span class=o>.</span><span class=n>bind</span><span class=p>()</span></span></span></code></pre></div></div><h2 id=deployment-on-aws>Deployment on AWS</h2><ol><li><strong>Cluster Setup</strong>: Use the Ray Cluster Launcher to provision EC2 instances. Define your cluster configuration in a <code>cluster.yaml</code> file, specifying the instance types (e.g., <code>g5.xlarge</code> for workers).</li><li><strong>Deploy</strong>: Run <code>ray up cluster.yaml</code> to start the cluster.</li><li><strong>Serve</strong>: Submit your serve application using <code>serve run serve_config.yaml</code>.</li></ol><h2 id=monitoring-and-optimization>Monitoring and Optimization</h2><p>Ray provides a built-in dashboard to monitor actor status, GPU usage, and request latency. For production, integrate this with <strong>Prometheus</strong> and <strong>Grafana</strong> to track:</p><ul><li><strong>Queue Latency</strong>: Time requests spend waiting for a replica.</li><li><strong>GPU Utilization</strong>: Ensure you aren&rsquo;t under-provisioning expensive hardware.</li><li><strong>Token Throughput</strong>: Measure tokens/second to benchmark performance.</li></ul><p>By leveraging Ray Serve with AWS, we create a flexible, scalable inference platform that avoids the vendor lock-in of managed services while providing full control over the serving infrastructure.</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2025-11-23</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on X" data-sharer=x data-url=https://histty.in/blog/deploying-llms-aws-ray-serve/ data-title="Scalable LLM Deployment on AWS with Ray Serve and FastAPI" data-via=iceadobe data-hashtags=devops><i class="fab fa-x-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Threads" data-sharer=threads data-url=https://histty.in/blog/deploying-llms-aws-ray-serve/ data-title="Scalable LLM Deployment on AWS with Ray Serve and FastAPI"><i class="fab fa-threads fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Linkedin" data-sharer=linkedin data-url=https://histty.in/blog/deploying-llms-aws-ray-serve/><i class="fab fa-linkedin fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://histty.in/blog/deploying-llms-aws-ray-serve/ data-title="Scalable LLM Deployment on AWS with Ray Serve and FastAPI"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Reddit" data-sharer=reddit data-url=https://histty.in/blog/deploying-llms-aws-ray-serve/><i class="fab fa-reddit fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://histty.in/blog/deploying-llms-aws-ray-serve/ data-title="Scalable LLM Deployment on AWS with Ray Serve and FastAPI"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@15.14.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://histty.in/blog/deploying-llms-aws-ray-serve/ data-title="Scalable LLM Deployment on AWS with Ray Serve and FastAPI"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Diaspora" data-sharer=diaspora data-url=https://histty.in/blog/deploying-llms-aws-ray-serve/ data-title="Scalable LLM Deployment on AWS with Ray Serve and FastAPI" data-description><i class="fab fa-diaspora fa-fw" aria-hidden=true></i></a><a href="https://t.me/share/url?url=https%3a%2f%2fhistty.in%2fblog%2fdeploying-llms-aws-ray-serve%2f&amp;text=Scalable%20LLM%20Deployment%20on%20AWS%20with%20Ray%20Serve%20and%20FastAPI" target=_blank title="Share on Telegram"><i class="fab fa-telegram fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/devops/>Devops</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>All content © 2025 Himalaya Ghimire. All rights reserved.</div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i></a></div><div id=fixed-buttons-hidden><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><script src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script src=https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js></script><script src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.2/sharer.min.js></script><script>window.config={comment:{},search:{highlightTag:"em",lunrIndexURL:"/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"}}</script><script src=/js/theme.min.js></script></body></html>